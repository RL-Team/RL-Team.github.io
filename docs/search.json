[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Markov Decision Processes\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html",
    "href": "2024-04-05-Markov-Decision-Processes.html",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "Reinforcement Learning is a sub-domain of machine learning where a learner called an agent interacts with its surroundings called environment. In return, the environment provides rewards and a new state determined by the actions of the agent.\n\n\n\n\nThe learner and decision-maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment.\n\nPlaceholder for Image\nThe environment refers to the aspects of the problem that the agent cannot directly control or manipulate. The agent’s actions, on the other hand, are the decisions that the agent is tasked with learning to make in order to maximize some reward signal.\nThe state of the environment encompasses all the information that may be useful for the agent in choosing its actions. Notably, the agent is not assumed to be completely ignorant of the environment. For example, the agent may have some knowledge about how its actions and the resulting states lead to the calculation of rewards. However, even though the agent understands this reward function, it is still considered part of the environment because the agent cannot arbitrarily change it.\nThis distinction between what the agent knows and what it can control is a critical concept in reinforcement learning. An agent may have a thorough understanding of the problem domain, much like a human who knows the rules of solving a Rubik’s cube but still struggles actually to find the optimal solution. The agent-environment relationship, therefore, represents the fundamental limitations on the agent’s control rather than just the limitations on its knowledge.\n\n\n\n\n“The future is independent of the past given the future.”\n\nThe agent makes its decisions as a function of a signal from the environment called the environment’s state. Ideally, a state signal should summarise past actions compactly yet in such a way that all relevant information is retained. This normally requires more than the immediate actions but never more than the complete history of all past sensations. A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property.\nMathematically, the Markov property can be expressed as,\nP[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1}, ..., S_{t}]\n\n\nA Markov Process is a memoryless random process, i.e. a sequence of random states \\(S_{1}, S_{2}, ...\\) with the Markov property. A Markov Process can be represented as a tuple \\(\\langle S, \\textit{P} \\rangle\\), where \\(S\\) is a finite set of states and P is the transition state probability matrix, P\\(ss'\\) \\(= P[S_{t+1} = s' | S_{t} = s]\\).\n\n\n\nFor Markov state \\(s\\) and successor state \\(s'\\), the state transition probability is defined by,\n\\textit{P}_{ss'} = P[S_{t+1} = s' | S_{t} = s]\nThe state transition matrix P defines transition probabilities from all states to all successor states.\n\\textit{P} = \\begin{bmatrix} \n    \\textit{P}_{11} & \\dots & \\textit{P}_{1n} \\\\\n    \\vdots & \\ddots & \\\\\n    \\textit{P}_{n1} &        & \\textit{P}_{nn} \n    \\end{bmatrix}\n\n\n\nImage Placeholder\n\n\n\n\n\nAll of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n\nIn simpler terms, it is a numerical value given to the agent based on some action at some state in the environment.\n\n\n\nIf the sequence of rewards received after time step \\(t\\) is denoted \\(R_{t+1}, R_{t+2}, R_{t+3}, . . .\\), we seek to maximize the expected return, where the return \\(G_t\\) is defined as some specific function of the reward sequence.\nG_t = R_{t+1} + R_{t+2} + R_{t+3} + · · · + R_T,\\text{ where T is a final time step}\n\n\n\n\nThe agent–environment interaction breaks naturally into subsequences, which we call episodes. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Tasks with episodes of this kind are called episodic tasks.\n\nHowever, this is not always the case.\n\nIn many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. We call these continuing tasks.\n\n\n\n\n\nThe discount rate determines the present value of future rewards.\n\nThe agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses \\(A_t\\) to maximize the expected discounted return:\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\(\\gamma\\) is a parameter, \\(0 \\leq \\gamma \\leq 1\\), called the discount rate. It determines the importance given to future rewards.\n\n\n\n\nA reinforcement learning task that satisfies the Markov property is called a Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP).\n\nA Markov Decision Process can be represented as a tuple \\(\\langle S, A, \\textit{P}, R, \\gamma \\rangle\\) where, - \\(S\\) is a finite set of states * \\(A\\) is a finite set of actions * P is a state transition matrix, \\(P^a_{ss'} = \\mathbb{P}\\left[S_{t+1} = s' | S_t = s, A_t = a\\right]\\) * \\(R\\) is a reward function, \\(R^a_s = \\mathbb{E}\\left[R_{t+1} | S_t = s, A_t = a\\right]\\) * \\(\\gamma\\) is a discount rate, \\(\\gamma \\in [0,1]\\)\n\n\n\n\nA policy \\(\\pi\\) is a probability distribution over actions given states.\n\n\\pi (a|s) = \\mathbb{P}\\left[A_t = a|S_t = s\\right]\n\nA policy defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.\n\n\n\n\n\nValue Functions estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of expected return.\n\n\n\nThe state-value function \\(V_{\\pi}(s)\\) of an MDP is the expected return starting from a state \\(s\\) under a policy \\(\\pi\\).\nV_{\\pi}(s) = \\mathbb{E}\\left[G_t|S_t = s\\right] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s \\right]\n\n\n\nThe action-value function \\(q_{\\pi}(s, a)\\) is the expected return starting from \\(s\\), taking the action \\(a\\), and thereafter following policy \\(\\pi\\).\nq_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\, \\middle| \\, S_t = s, A_t = a \\right] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s, A_t = a \\right]\n\n\n\n\n[1]: Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.\n[2]: Markov Decision Processes, Subir Varma."
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#the-agentenvironment-interface",
    "href": "2024-04-05-Markov-Decision-Processes.html#the-agentenvironment-interface",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "The learner and decision-maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment.\n\nPlaceholder for Image\nThe environment refers to the aspects of the problem that the agent cannot directly control or manipulate. The agent’s actions, on the other hand, are the decisions that the agent is tasked with learning to make in order to maximize some reward signal.\nThe state of the environment encompasses all the information that may be useful for the agent in choosing its actions. Notably, the agent is not assumed to be completely ignorant of the environment. For example, the agent may have some knowledge about how its actions and the resulting states lead to the calculation of rewards. However, even though the agent understands this reward function, it is still considered part of the environment because the agent cannot arbitrarily change it.\nThis distinction between what the agent knows and what it can control is a critical concept in reinforcement learning. An agent may have a thorough understanding of the problem domain, much like a human who knows the rules of solving a Rubik’s cube but still struggles actually to find the optimal solution. The agent-environment relationship, therefore, represents the fundamental limitations on the agent’s control rather than just the limitations on its knowledge."
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#the-markov-property",
    "href": "2024-04-05-Markov-Decision-Processes.html#the-markov-property",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "“The future is independent of the past given the future.”\n\nThe agent makes its decisions as a function of a signal from the environment called the environment’s state. Ideally, a state signal should summarise past actions compactly yet in such a way that all relevant information is retained. This normally requires more than the immediate actions but never more than the complete history of all past sensations. A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property.\nMathematically, the Markov property can be expressed as,\nP[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1}, ..., S_{t}]\n\n\nA Markov Process is a memoryless random process, i.e. a sequence of random states \\(S_{1}, S_{2}, ...\\) with the Markov property. A Markov Process can be represented as a tuple \\(\\langle S, \\textit{P} \\rangle\\), where \\(S\\) is a finite set of states and P is the transition state probability matrix, P\\(ss'\\) \\(= P[S_{t+1} = s' | S_{t} = s]\\).\n\n\n\nFor Markov state \\(s\\) and successor state \\(s'\\), the state transition probability is defined by,\n\\textit{P}_{ss'} = P[S_{t+1} = s' | S_{t} = s]\nThe state transition matrix P defines transition probabilities from all states to all successor states.\n\\textit{P} = \\begin{bmatrix} \n    \\textit{P}_{11} & \\dots & \\textit{P}_{1n} \\\\\n    \\vdots & \\ddots & \\\\\n    \\textit{P}_{n1} &        & \\textit{P}_{nn} \n    \\end{bmatrix}\n\n\n\nImage Placeholder"
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#reward",
    "href": "2024-04-05-Markov-Decision-Processes.html#reward",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n\nIn simpler terms, it is a numerical value given to the agent based on some action at some state in the environment."
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#return",
    "href": "2024-04-05-Markov-Decision-Processes.html#return",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "If the sequence of rewards received after time step \\(t\\) is denoted \\(R_{t+1}, R_{t+2}, R_{t+3}, . . .\\), we seek to maximize the expected return, where the return \\(G_t\\) is defined as some specific function of the reward sequence.\nG_t = R_{t+1} + R_{t+2} + R_{t+3} + · · · + R_T,\\text{ where T is a final time step}"
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#episodic-and-continuing-tasks",
    "href": "2024-04-05-Markov-Decision-Processes.html#episodic-and-continuing-tasks",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "The agent–environment interaction breaks naturally into subsequences, which we call episodes. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Tasks with episodes of this kind are called episodic tasks.\n\nHowever, this is not always the case.\n\nIn many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. We call these continuing tasks."
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#discount-rate",
    "href": "2024-04-05-Markov-Decision-Processes.html#discount-rate",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "The discount rate determines the present value of future rewards.\n\nThe agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses \\(A_t\\) to maximize the expected discounted return:\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\(\\gamma\\) is a parameter, \\(0 \\leq \\gamma \\leq 1\\), called the discount rate. It determines the importance given to future rewards."
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#markov-decision-processes-1",
    "href": "2024-04-05-Markov-Decision-Processes.html#markov-decision-processes-1",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "A reinforcement learning task that satisfies the Markov property is called a Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP).\n\nA Markov Decision Process can be represented as a tuple \\(\\langle S, A, \\textit{P}, R, \\gamma \\rangle\\) where, - \\(S\\) is a finite set of states * \\(A\\) is a finite set of actions * P is a state transition matrix, \\(P^a_{ss'} = \\mathbb{P}\\left[S_{t+1} = s' | S_t = s, A_t = a\\right]\\) * \\(R\\) is a reward function, \\(R^a_s = \\mathbb{E}\\left[R_{t+1} | S_t = s, A_t = a\\right]\\) * \\(\\gamma\\) is a discount rate, \\(\\gamma \\in [0,1]\\)"
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#policies",
    "href": "2024-04-05-Markov-Decision-Processes.html#policies",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "A policy \\(\\pi\\) is a probability distribution over actions given states.\n\n\\pi (a|s) = \\mathbb{P}\\left[A_t = a|S_t = s\\right]\n\nA policy defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states."
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#value-function",
    "href": "2024-04-05-Markov-Decision-Processes.html#value-function",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "Value Functions estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of expected return.\n\n\n\nThe state-value function \\(V_{\\pi}(s)\\) of an MDP is the expected return starting from a state \\(s\\) under a policy \\(\\pi\\).\nV_{\\pi}(s) = \\mathbb{E}\\left[G_t|S_t = s\\right] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s \\right]\n\n\n\nThe action-value function \\(q_{\\pi}(s, a)\\) is the expected return starting from \\(s\\), taking the action \\(a\\), and thereafter following policy \\(\\pi\\).\nq_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\, \\middle| \\, S_t = s, A_t = a \\right] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s, A_t = a \\right]"
  },
  {
    "objectID": "2024-04-05-Markov-Decision-Processes.html#references",
    "href": "2024-04-05-Markov-Decision-Processes.html#references",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "[1]: Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.\n[2]: Markov Decision Processes, Subir Varma."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html",
    "href": "blogs/Markov-Decision-Processes.html",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "Reinforcement Learning is a sub-domain of machine learning where a learner called an agent interacts with its surroundings called environment. In return, the environment provides rewards and a new state determined by the actions of the agent.\n\n\n\n\nThe learner and decision-maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment.\n\nPlaceholder for Image\nThe environment refers to the aspects of the problem that the agent cannot directly control or manipulate. The agent’s actions, on the other hand, are the decisions that the agent is tasked with learning to make in order to maximize some reward signal.\nThe state of the environment encompasses all the information that may be useful for the agent in choosing its actions. Notably, the agent is not assumed to be completely ignorant of the environment. For example, the agent may have some knowledge about how its actions and the resulting states lead to the calculation of rewards. However, even though the agent understands this reward function, it is still considered part of the environment because the agent cannot arbitrarily change it.\nThis distinction between what the agent knows and what it can control is a critical concept in reinforcement learning. An agent may have a thorough understanding of the problem domain, much like a human who knows the rules of solving a Rubik’s cube but still struggles actually to find the optimal solution. The agent-environment relationship, therefore, represents the fundamental limitations on the agent’s control rather than just the limitations on its knowledge.\n\n\n\n\n“The future is independent of the past given the future.”\n\nThe agent makes its decisions as a function of a signal from the environment called the environment’s state. Ideally, a state signal should summarise past actions compactly yet in such a way that all relevant information is retained. This normally requires more than the immediate actions but never more than the complete history of all past sensations. A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property.\nMathematically, the Markov property can be expressed as,\n\\[\nP[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1}, ..., S_{t}]\n\\]\n\n\nA Markov Process is a memoryless random process, i.e. a sequence of random states \\(S_{1}, S_{2}, ...\\) with the Markov property. A Markov Process can be represented as a tuple \\(\\langle S, \\textit{P} \\rangle\\), where \\(S\\) is a finite set of states and P is the transition state probability matrix, P\\(ss'\\) \\(= P[S_{t+1} = s' | S_{t} = s]\\).\n\n\n\nFor Markov state \\(s\\) and successor state \\(s'\\), the state transition probability is defined by,\n\\[ {P}_{ss'} = P[S_{t+1} = s' | S_{t} = s] \\]\nThe state transition matrix P defines transition probabilities from all states to all successor states.\n\\[\n{P} = \\begin{bmatrix}\n    \\textit{P}_{11} & \\dots & \\textit{P}_{1n} \\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\textit{P}_{n1} & \\dots & \\textit{P}_{nn}\n    \\end{bmatrix}\n\\] ### Example of Markov chain\nImage Placeholder\n\n\n\n\n\nAll of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n\nIn simpler terms, it is a numerical value given to the agent based on some action at some state in the environment.\n\n\n\nIf the sequence of rewards received after time step \\(t\\) is denoted \\(R_{t+1}, R_{t+2}, R_{t+3}, . . .\\), we seek to maximize the expected return, where the return \\(G_t\\) is defined as some specific function of the reward sequence.\n\\[ G_t = R_{t+1} + R_{t+2} + R_{t+3} + · · · + R_T,\\text{ where T is a final time step} \\]\n\n\n\n\nThe agent–environment interaction breaks naturally into subsequences, which we call episodes. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Tasks with episodes of this kind are called episodic tasks.\n\nHowever, this is not always the case.\n\nIn many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. We call these continuing tasks.\n\n\n\n\n\nThe discount rate determines the present value of future rewards.\n\nThe agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses \\(A_t\\) to maximize the expected discounted return:\n\\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\]\n\\(\\gamma\\) is a parameter, \\(0 \\leq \\gamma \\leq 1\\), called the discount rate. It determines the importance given to future rewards.\n\n\n\n\nA reinforcement learning task that satisfies the Markov property is called a Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP).\n\nA Markov Decision Process can be represented as a tuple \\(\\langle S, A, \\textit{P}, R, \\gamma \\rangle\\) where, - \\(S\\) is a finite set of states * \\(A\\) is a finite set of actions * P is a state transition matrix, \\(P^a_{ss'} = \\mathbb{P}\\left[S_{t+1} = s' | S_t = s, A_t = a\\right]\\) * \\(R\\) is a reward function, \\(R^a_s = \\mathbb{E}\\left[R_{t+1} | S_t = s, A_t = a\\right]\\) * \\(\\gamma\\) is a discount rate, \\(\\gamma \\in [0,1]\\)\n\n\n\n\nA policy \\(\\pi\\) is a probability distribution over actions given states.\n\n\\[ \\pi (a|s) = \\mathbb{P}\\left[A_t = a|S_t = s\\right] \\]\n\nA policy defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.\n\n\n\n\n\nValue Functions estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of expected return.\n\n\n\nThe state-value function \\(V_{\\pi}(s)\\) of an MDP is the expected return starting from a state \\(s\\) under a policy \\(\\pi\\). \\[ V_{\\pi}(s) = \\mathbb{E}\\left[G_t|S_t = s\\right] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s \\right] \\]\n\n\n\nThe action-value function \\(q_{\\pi}(s, a)\\) is the expected return starting from \\(s\\), taking the action \\(a\\), and thereafter following policy \\(\\pi\\).\n\\[ q_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\, \\middle| \\, S_t = s, A_t = a \\right] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s, A_t = a \\right] \\]\n\n\n\n\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.\nMarkov Decision Processes, Subir Varma\nJ. Norris, “Markov Chains”, Cambridge University Press, Cambridge, 1998."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#the-agentenvironment-interface",
    "href": "blogs/Markov-Decision-Processes.html#the-agentenvironment-interface",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "The learner and decision-maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment.\n\nPlaceholder for Image\nThe environment refers to the aspects of the problem that the agent cannot directly control or manipulate. The agent’s actions, on the other hand, are the decisions that the agent is tasked with learning to make in order to maximize some reward signal.\nThe state of the environment encompasses all the information that may be useful for the agent in choosing its actions. Notably, the agent is not assumed to be completely ignorant of the environment. For example, the agent may have some knowledge about how its actions and the resulting states lead to the calculation of rewards. However, even though the agent understands this reward function, it is still considered part of the environment because the agent cannot arbitrarily change it.\nThis distinction between what the agent knows and what it can control is a critical concept in reinforcement learning. An agent may have a thorough understanding of the problem domain, much like a human who knows the rules of solving a Rubik’s cube but still struggles actually to find the optimal solution. The agent-environment relationship, therefore, represents the fundamental limitations on the agent’s control rather than just the limitations on its knowledge."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#the-markov-property",
    "href": "blogs/Markov-Decision-Processes.html#the-markov-property",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "“The future is independent of the past given the future.”\n\nThe agent makes its decisions as a function of a signal from the environment called the environment’s state. Ideally, a state signal should summarise past actions compactly yet in such a way that all relevant information is retained. This normally requires more than the immediate actions but never more than the complete history of all past sensations. A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property.\nMathematically, the Markov property can be expressed as,\n\\[\nP[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1}, ..., S_{t}]\n\\]\n\n\nA Markov Process is a memoryless random process, i.e. a sequence of random states \\(S_{1}, S_{2}, ...\\) with the Markov property. A Markov Process can be represented as a tuple \\(\\langle S, \\textit{P} \\rangle\\), where \\(S\\) is a finite set of states and P is the transition state probability matrix, P\\(ss'\\) \\(= P[S_{t+1} = s' | S_{t} = s]\\).\n\n\n\nFor Markov state \\(s\\) and successor state \\(s'\\), the state transition probability is defined by,\n\\[ {P}_{ss'} = P[S_{t+1} = s' | S_{t} = s] \\]\nThe state transition matrix P defines transition probabilities from all states to all successor states.\n\\[\n{P} = \\begin{bmatrix}\n    \\textit{P}_{11} & \\dots & \\textit{P}_{1n} \\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\textit{P}_{n1} & \\dots & \\textit{P}_{nn}\n    \\end{bmatrix}\n\\] ### Example of Markov chain\nImage Placeholder"
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#reward",
    "href": "blogs/Markov-Decision-Processes.html#reward",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n\nIn simpler terms, it is a numerical value given to the agent based on some action at some state in the environment."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#return",
    "href": "blogs/Markov-Decision-Processes.html#return",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "If the sequence of rewards received after time step \\(t\\) is denoted \\(R_{t+1}, R_{t+2}, R_{t+3}, . . .\\), we seek to maximize the expected return, where the return \\(G_t\\) is defined as some specific function of the reward sequence.\n\\[ G_t = R_{t+1} + R_{t+2} + R_{t+3} + · · · + R_T,\\text{ where T is a final time step} \\]"
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#episodic-and-continuing-tasks",
    "href": "blogs/Markov-Decision-Processes.html#episodic-and-continuing-tasks",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "The agent–environment interaction breaks naturally into subsequences, which we call episodes. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Tasks with episodes of this kind are called episodic tasks.\n\nHowever, this is not always the case.\n\nIn many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. We call these continuing tasks."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#discount-rate",
    "href": "blogs/Markov-Decision-Processes.html#discount-rate",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "The discount rate determines the present value of future rewards.\n\nThe agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses \\(A_t\\) to maximize the expected discounted return:\n\\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\]\n\\(\\gamma\\) is a parameter, \\(0 \\leq \\gamma \\leq 1\\), called the discount rate. It determines the importance given to future rewards."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#markov-decision-processes-1",
    "href": "blogs/Markov-Decision-Processes.html#markov-decision-processes-1",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "A reinforcement learning task that satisfies the Markov property is called a Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP).\n\nA Markov Decision Process can be represented as a tuple \\(\\langle S, A, \\textit{P}, R, \\gamma \\rangle\\) where, - \\(S\\) is a finite set of states * \\(A\\) is a finite set of actions * P is a state transition matrix, \\(P^a_{ss'} = \\mathbb{P}\\left[S_{t+1} = s' | S_t = s, A_t = a\\right]\\) * \\(R\\) is a reward function, \\(R^a_s = \\mathbb{E}\\left[R_{t+1} | S_t = s, A_t = a\\right]\\) * \\(\\gamma\\) is a discount rate, \\(\\gamma \\in [0,1]\\)"
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#policies",
    "href": "blogs/Markov-Decision-Processes.html#policies",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "A policy \\(\\pi\\) is a probability distribution over actions given states.\n\n\\[ \\pi (a|s) = \\mathbb{P}\\left[A_t = a|S_t = s\\right] \\]\n\nA policy defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#value-function",
    "href": "blogs/Markov-Decision-Processes.html#value-function",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "Value Functions estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of expected return.\n\n\n\nThe state-value function \\(V_{\\pi}(s)\\) of an MDP is the expected return starting from a state \\(s\\) under a policy \\(\\pi\\). \\[ V_{\\pi}(s) = \\mathbb{E}\\left[G_t|S_t = s\\right] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s \\right] \\]\n\n\n\nThe action-value function \\(q_{\\pi}(s, a)\\) is the expected return starting from \\(s\\), taking the action \\(a\\), and thereafter following policy \\(\\pi\\).\n\\[ q_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\, \\middle| \\, S_t = s, A_t = a \\right] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s, A_t = a \\right] \\]"
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#references",
    "href": "blogs/Markov-Decision-Processes.html#references",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.\nMarkov Decision Processes, Subir Varma\nJ. Norris, “Markov Chains”, Cambridge University Press, Cambridge, 1998."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RL-Team",
    "section": "",
    "text": "This is the homepage for the RL-Team. We are a group of enturepeneurs, students and faculty members who are interested in Reinforcement Learning.\nCome join us in our learning journey!"
  }
]