[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Policy Iteration and Value Iteration\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMarkov Decision Processes\n\n\n\n\n\n\n\nFundamentals\n\n\nReinforcement Learning\n\n\nMarkov Decision Processes\n\n\nMathematics\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2024\n\n\n\n\n\n\n  \n\n\n\n\nBellman Expectation Equation and Optimality\n\n\n\n\n\n\n\nFundamentals\n\n\nReinforcement Learning\n\n\nBellman Expectation Equation\n\n\nOptimality\n\n\nMathematics\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/Bellman-Expectation-Equation.html",
    "href": "blogs/Bellman-Expectation-Equation.html",
    "title": "Bellman Expectation Equation and Optimality",
    "section": "",
    "text": "How do we calculate the value functions \\(v_\\pi (s)\\) and \\(q_\\pi (s,a)\\), given a policy \\(\\pi\\)?"
  },
  {
    "objectID": "blogs/Bellman-Expectation-Equation.html#bellman-expectation-equation",
    "href": "blogs/Bellman-Expectation-Equation.html#bellman-expectation-equation",
    "title": "Bellman Expectation Equation and Optimality",
    "section": "Bellman Expectation Equation",
    "text": "Bellman Expectation Equation\nThe state-value function can be decomposed into immediate reward plus the discounted value of the successor state,\n\\[ v_\\pi(s) = \\mathbb{E}_\\pi [R_{t+1} + \\gamma V_\\pi(S_{t+1}) | S_t = s] \\]\nThe above equation is the Bellman expectation equation for the state-value function.\nThe action-value function similarly decomposed,\n\\[ q_\\pi(s, a) = \\mathbb{E}\\pi [R{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \\]\nThe above equation is the Bellman expectation equation for the action-value function.\nBack-up diagram state value\nThis backup diagram describes the value of being in a particular state.\nMathematically, this can be represented as,\n\\[ v(s) = R_s + \\gamma \\sum_{s' \\in S} P_{ss'} v(s') \\]\nSimilarly,\nbackup diagram for action value\nThis backup diagram shows us “how good” it is to take an action in a given state under a given policy.\nMathematically, this can be represented as,\n\\[ q_\\pi(s, a) = R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi(s')$ \\]\nIf we connect and extend both the back-up diagrams to further define \\(v_\\pi (s)\\),\nextended state value backup diagram\nMathematically, this can be represented as,\n\\[ v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\left(R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi(s')\\right) \\]\nExample:\nExample\nSimilarly, if we connect and extend both the back-up diagrams to further define \\(q_\\pi (s, a)\\),\nextended action value backup diagram\nMathematically, this can be represented as,\n\\[ q_{\\pi}(s, a) = R_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a'|s') q_{\\pi}(s', a') \\]\nExample:\nExample"
  },
  {
    "objectID": "blogs/Bellman-Expectation-Equation.html#optimal-value-function",
    "href": "blogs/Bellman-Expectation-Equation.html#optimal-value-function",
    "title": "Bellman Expectation Equation and Optimality",
    "section": "Optimal Value Function",
    "text": "Optimal Value Function\nThe Bellman Expectation Equations evaluate how good a state is for a particular policy. However, they do not tell us the optimal policy.\n\nOptimal State Value Function\nThe optimal state-value function \\(v_{*} (s)\\) is the maximum state-value function for all policies\n\\[ v_{*}(s) = \\mathop{\\max}_\\pi v_\\pi(s) \\]\n\\(v_{*} (s)\\) tells us the maximum reward that can be extracted from the system when starting in state s. However, it doesn’t tell us what policy to follow.\n\n\nOptimal Action Value Function\nThe optimal action-value function \\(q_{*} (s, a)\\) is the maximum action-value function for all policies\n\\[ q_{*}(s, a) = \\mathop{\\max}_\\pi q_\\pi(s, a) \\]\n\\(q_{*} (s, a)\\) tells us the maximum reward that can be extracted from the system if action a is taken while in state s."
  },
  {
    "objectID": "blogs/Bellman-Expectation-Equation.html#optimal-policy",
    "href": "blogs/Bellman-Expectation-Equation.html#optimal-policy",
    "title": "Bellman Expectation Equation and Optimality",
    "section": "Optimal Policy",
    "text": "Optimal Policy\nIn order to find an optimal policy, we have to define the notion of optimality. What does it mean for one policy to be better than another?\nA partial ordering over policies can be defined using the concept of value functions. The partial ordering states that one policy is better than another if the value function of the first policy is greater than or equal to the value function of the second policy for all states. This partial ordering relationship is formally expressed in the equation,\n\\[ \\pi \\geq \\pi' \\text{ if } v_\\pi(s) \\geq v_{\\pi'}(s), \\forall s \\]\n\nTheorem:\nFor any Markov Decision Process, 1. There exists an optimal policy \\(\\pi_{\\ast}\\) that is better than or equal to all other policies, \\(\\pi_{\\ast} \\geq \\pi, \\forall \\pi\\) 2. All optimal policies achieve the optimal value function, \\(v_{\\pi_{\\ast}}(s) = v_{\\ast}(s)\\) 3. All optimal policies achieve the optimal action-value function, \\(q_{\\pi_{\\ast}}(s, a) = q_{\\ast}(s, a)\\)\n\n\nFinding an Optimal Policy:\nAn optimal policy can be found by maximising over \\(q_{\\ast}(s, a)\\),\n\\[ \\pi_{\\ast}(a|s) = \\begin{cases}\n1 & \\text{if } a = \\arg\\underset{a \\in \\mathcal{A}}\\max q_*(s, a) \\\\\n0 & \\text{otherwise}\n\\end{cases} \\]\n\nThere is always a deterministic optimal policy for any MDP.\nIf we know \\(q_{\\ast}(s,a )\\), we immediately have the optimal policy.\n\nExample: EXAMPLE IMAGE"
  },
  {
    "objectID": "blogs/Bellman-Expectation-Equation.html#bellman-optimality-equation",
    "href": "blogs/Bellman-Expectation-Equation.html#bellman-optimality-equation",
    "title": "Bellman Expectation Equation and Optimality",
    "section": "Bellman Optimality Equation",
    "text": "Bellman Optimality Equation\nBack Diagram state-value\nSuppose the agent is in state S, and from that state, it can take two actions (a). Instead of using the Bellman Expectation Equation to calculate the value of being in state S by taking the average of the action-values, the agent chooses the action with the greater \\(q_{\\ast}\\) value. This gives the agent the value of being in state s.\n\\[ v_{\\ast}(s) = \\mathop{\\max}_a q_{\\ast}(s, a) \\]\nSimilarly,\nBackup diagram action-value\nSuppose the agent has taken an action a in some state s. The environment then transitions the agent to a new state s’, which could be any of the possible next states. In this case, rather than using the Bellman Expectation Equation, which would involve taking the average of the values of the possible next states, the agent uses the Bellman Optimality Equation.\n\\[ q_{\\ast}(s, a) = R_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a v_{\\ast}(s') \\]\nIf we connect and extend both the back-up diagrams,\nextended state value backup diagram\nAn agent in a particular state ‘s’ will take an action a based on a policy that assigns weighted probabilities to the available actions. This action then leads the agent to end up in one of several possible next states s’, with the probabilities of ending up in each state s determined by the weighted environment. To find the value of being in the original state s, we simply average the optimal values of all the possible next states s’. This average gives us the overall value of being in state s - it tells us how good it is for the agent to be in that state, taking into account the weighted probabilities of the available actions and the resulting next states.\n\\[ v_{\\ast}(s) = \\mathop{\\max}_a (R_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a v_{\\ast}(s')) \\]\nThere is no dependency on the policy anymore; it is solely a function of the environment’s randomness.\nextended action value backup diagram\nSuppose our agent is in a particular state s. The agent takes an action a in that state, which may result in the agent ending up in any of several possible next states s’. From each of these possible next states, the agent wants to identify the action with the highest \\(q_{\\ast}\\) value, i.e. the action that will maximize the expected future reward. The agent then backs this information up to the current state s, which allows the agent to determine the overall value of taking the original action a from the current state. In this way, the agent is able to choose the action that will lead to the highest expected future reward by considering the possible future states and the best actions to take in each of them.\n\\[ q_{\\ast}(s, a) = R_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a \\mathop{\\max}_{a'}q_{\\ast}(s', a') \\]\nExample"
  },
  {
    "objectID": "blogs/Bellman-Expectation-Equation.html#references",
    "href": "blogs/Bellman-Expectation-Equation.html#references",
    "title": "Bellman Expectation Equation and Optimality",
    "section": "References",
    "text": "References\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.\nMarkov Decision Processes, Subir Varma.\nMarkov Decision Processes, David Silver, UCL Course on RL.\nJ. Norris, “Markov Chains,” Cambridge University Press, Cambridge, 1998."
  },
  {
    "objectID": "blogs/Policy-Iteration-and-Value-Iteration.html",
    "href": "blogs/Policy-Iteration-and-Value-Iteration.html",
    "title": "Policy Iteration and Value Iteration",
    "section": "",
    "text": "The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).\n\nDynamic programming involves solving complex problems by breaking them down into sub-problems. Dynamic programming is a very general solution method for problems that have two properties,\n\nOptimal Substructure: The optimal solution to a dynamic optimisation problem can be found by combining the optimal solutions to its sub-problems. This is known as the Principle of Optimality. Optimal solutions can be decomposed into subproblems.\nOverlapping subproblems: Sub-problems can recur many times. The solution of one sub-problem is cached and reused to solve recursive sub-problems.\n\nMarkov decision processes satisfy both properties,\n\nBellman equation gives recursive decomposition. The equation breaks down finding the value function of a state by dividing it into sub-problems.\nv_\\pi(s) = \\mathbb{E}_\\pi [R_{t+1} + \\gamma V_\\pi(S_{t+1}) | S_t = s]\nValue function stores and reuses solutions.\n\n\n\nDynamic programming assumes full knowledge of the MDP. It is used for planning in an MDP.\nFor Prediction, - Input: MDP \\(\\langle S, A, \\textit{P}, R, \\gamma \\rangle\\) and policy \\(\\pi\\) OR MRP \\(\\langle S, \\textit{P}^{\\pi}, R^{\\pi}, \\gamma \\rangle\\) - Output: value function \\(v_\\pi\\)\nFor Control: - Input: MDP \\(\\langle S, A, \\textit{P}, R, \\gamma \\rangle\\) - Output: optimal value function \\(v_{\\ast}\\) and optimal policy \\(\\pi_{\\ast}\\).\n\n\n\n\nHow do we evaluate an arbitrary policy \\(\\pi\\)?\nWe apply the Bellman Expectation Equation with iterative backups. We calculate the value of the next state by backing up the value of the current state from the previous iteration.\nv_{k+1}(s) = \\sum_{a \\in A} \\pi(a|s) \\left( R_{s}^a + \\gamma \\sum_{s' \\in S} P_{ss'}^{a} v_k(s') \\right)\nAt each iteration \\(k + 1\\), update \\(v_{k + 1}(s)\\) from \\(v_{k}(s')\\), for all states \\(s \\in \\mathcal{S}\\). Here, \\(s'\\) is a successor state of \\(s\\).\nBackup diagram\n\n\nImage1\nThe gridworld consists of a 4x4 grid with 15 states, where each cell represents a state. There is one terminal state (shown as shaded squares), and the agent receives a reward of -1 until it reaches the terminal state. The agent follows a uniform random policy, where it has an equal probability of 0.25 to move in any of the four directions (up, down, left, or right) from each state.\n\\pi(n|.) = \\pi(e|.) = \\pi(s|.) = \\pi(w|.) = 0.25\nActions that would lead the agent out of the grid leave the state unchanged. The goal is to evaluate the performance of this random policy in terms of the undiscounted episodic MDP, where the discount factor \\(\\gamma\\) is set to 1, indicating that future rewards are not discounted.\nImage 2\n\n\n\n\nGiven an initial policy \\(\\pi\\), the process of policy improvement involves evaluating the policy and then improving it iteratively. The policy evaluation step estimates the value function \\(v_{\\pi}(s)\\) for each state s under the current policy \\(\\pi\\).\nv_{\\pi}(s) = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + ... | S_t = s]\nThe policy improvement step then updates the policy to a new policy \\(\\pi'\\) by acting greedily with respect to the value function \\(v_{\\pi}\\).\n\\pi' = greedy(v_{\\pi})\nDiagram\nIn the gridworld example, the policy was optimal, \\(\\pi' = \\pi_{\\ast}\\). The process of policy iteration always converges to \\(\\pi_{\\ast}\\).\n\n\nThe problem involves managing a car rental business with two locations, where each location has a maximum capacity of 20 cars. The states in this problem represent the number of cars available at each location. The actions involve moving up to 5 cars between the two locations overnight to rebalance the inventory based on demand. The reward is \\(\\$10\\) for each car rented (assuming a car is available at the requested location). The transitions, or the dynamics of the environment, are determined by the random returns and requests for cars at each location. These transitions follow a Poisson distribution, n returns/requests with prob \\(\\frac{{e^{-\\lambda } \\lambda^n }}{{n!}}\\), where: - At the first location, the average number of car returns and requests is 3. - At the second location, the average number of car requests is 4, and the average number of car returns is 2.\nThe goal is to find an optimal policy for moving cars between the two locations to maximize the overall expected reward (revenue from rented cars).\nDiagram\nThe plots show the progression of value functions \\((v_0, v_1, v_2)\\) and intermediate policies \\((\\pi_0, \\pi_1, \\pi_2, \\pi_3, \\pi_4)\\) during the iterations of policy evaluation and improvement.\nThe value functions are depicted as 3D surfaces, representing the expected future rewards for each combination of car counts at the two locations. The policies are visualized as 2D plots, indicating the optimal number of cars to move between locations for each state.\nStarting with an initial policy π0, the process alternates between evaluating the current policy to compute the value function and improving the policy greedily based on the value function.\nThe iterations continue until the policy converges to the optimal policy \\(\\pi^{\\ast}\\) that maximizes the expected rewards in the Car Rental problem.\n\n\n\nConsider a deterministic policy, $a = (s). We can improve the policy by acting greedily,\n\\pi'(s) = \\arg\\max_{a\\in\\mathcal{A}} q_\\pi(s, a)\nThis improves the value from any state \\(s\\) over one step,\nq_\\pi(s, \\pi'(s)) = \\max_{a\\in\\mathcal{A}} q_\\pi(s, a) \\geq q_\\pi(s, \\pi(s)) = v_\\pi(s)\nIt therefore improves the value function, \\(v_{\\pi'}(s) \\geq v_\\pi(s)\\)\n\\begin{align*}\nv_\\pi(s) &\\leq q_\\pi(s, \\pi'(s)) = \\mathbb{E}\\pi[R{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\\n&\\leq \\mathbb{E}\\pi[R{t+1} + \\gamma q_\\pi(S_{t+1},\\pi'(S_{t+1})) | S_t = s] \\\n&\\leq \\mathbb{E}\\pi[R{t+1} + \\gamma R_{t+2} + \\gamma^2 q_\\pi(S_{t+2},\\pi'(S_{t+2})) | S_t = s] \\\n&\\leq \\mathbb{E}\\pi[R{t+1} + \\gamma R_{t+2} + \\ldots | S_t = s] = v_{\\pi'}(s)\n\\end{align*}\nIf improvements stop,\nq_\\pi(s, \\pi'(s)) = \\max_{a\\in\\mathcal{A}} q_\\pi(s, a) = q_\\pi(s, \\pi(s)) = v_\\pi(s)\nThen, the Bellman optimality equation has been satisfied\nv_\\pi(s) = \\max_{a\\in\\mathcal{A}} q_\\pi(s, a)\nTherefore \\(v_\\pi(s) = v_*(s)\\) for all \\(s \\in \\mathcal{S}\\). So \\(\\pi\\) is an optimal policy.\n\n\n\n\n\n\nAny optimal policy can be subdivided into two components: - An optimal first action \\(A\\), - Followed by an optimal policy from successor state \\(S'\\)\n\nTheorem (Principle of Optimality)\nA policy \\(\\pi(a|s)\\) achieves the optimal value from state \\(s\\), \\(v_\\pi(s) = v_(s)\\), if and only if - For any state \\(s'\\) reachable from \\(s\\) - \\(\\pi\\) achieves the optimal value from state \\(s'\\), \\(v_\\pi(s') = v_(s')\\)\n\n\n\n\nValue iteration is a technique used to compute the optimal value function v*(s) for a given Markov Decision Process (MDP). The idea is to iteratively update the value function using the Bellman equation until convergence.\nIf we know the solution to the subproblems \\(v_{\\ast}(s')\\), i.e., the optimal values for the next states \\(s'\\), then the optimal value \\(v_{\\ast}(s)\\) for the current state s can be found by a one-step lookahead:\nv_{\\ast}(s) \\leftarrow \\max_{a \\in A} R^a_s + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_{\\ast}(s')\nThe value iteration algorithm applies these updates iteratively, starting with an initial approximation and updating the values until convergence.\nThe intuition behind value iteration is to start with the final rewards and work backwards, computing the optimal values by considering the immediate rewards and the discounted future rewards from the next states.\nAlthough derived for deterministic MDPs, value iteration can also be applied to stochastic (loopy) MDPs.\n\n\n\nProblem: Find the optimal policy \\(\\pi\\)\nWe apply the Bellman Optimality Equation with iterative backups.\nv_{k + 1}(s) = \\max_{a \\in A} \\left[R_{s}^a + \\gamma \\sum_{s' \\in S}  P_{ss'}^a v_{k}(s')\\right]\nAt each iteration \\(k + 1\\), update \\(v_{k + 1}(s)\\) from \\(v_{k}(s')\\), for all states \\(s \\in \\mathcal{S}\\), using synchronous backups.\nDiagram\n\n\n\n\nImage\n\n\n\n[1]: Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.\n[2]: Markov Decision Processes, Subir Varma.\n[3]: Markov Decision Processes, David Silver, UCL Course on RL."
  },
  {
    "objectID": "blogs/Policy-Iteration-and-Value-Iteration.html#dynamic-programming",
    "href": "blogs/Policy-Iteration-and-Value-Iteration.html#dynamic-programming",
    "title": "Policy Iteration and Value Iteration",
    "section": "",
    "text": "The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).\n\nDynamic programming involves solving complex problems by breaking them down into sub-problems. Dynamic programming is a very general solution method for problems that have two properties,\n\nOptimal Substructure: The optimal solution to a dynamic optimisation problem can be found by combining the optimal solutions to its sub-problems. This is known as the Principle of Optimality. Optimal solutions can be decomposed into subproblems.\nOverlapping subproblems: Sub-problems can recur many times. The solution of one sub-problem is cached and reused to solve recursive sub-problems.\n\nMarkov decision processes satisfy both properties,\n\nBellman equation gives recursive decomposition. The equation breaks down finding the value function of a state by dividing it into sub-problems.\nv_\\pi(s) = \\mathbb{E}_\\pi [R_{t+1} + \\gamma V_\\pi(S_{t+1}) | S_t = s]\nValue function stores and reuses solutions.\n\n\n\nDynamic programming assumes full knowledge of the MDP. It is used for planning in an MDP.\nFor Prediction, - Input: MDP \\(\\langle S, A, \\textit{P}, R, \\gamma \\rangle\\) and policy \\(\\pi\\) OR MRP \\(\\langle S, \\textit{P}^{\\pi}, R^{\\pi}, \\gamma \\rangle\\) - Output: value function \\(v_\\pi\\)\nFor Control: - Input: MDP \\(\\langle S, A, \\textit{P}, R, \\gamma \\rangle\\) - Output: optimal value function \\(v_{\\ast}\\) and optimal policy \\(\\pi_{\\ast}\\)."
  },
  {
    "objectID": "blogs/Policy-Iteration-and-Value-Iteration.html#iterative-policy-evaluation",
    "href": "blogs/Policy-Iteration-and-Value-Iteration.html#iterative-policy-evaluation",
    "title": "Policy Iteration and Value Iteration",
    "section": "",
    "text": "How do we evaluate an arbitrary policy \\(\\pi\\)?\nWe apply the Bellman Expectation Equation with iterative backups. We calculate the value of the next state by backing up the value of the current state from the previous iteration.\nv_{k+1}(s) = \\sum_{a \\in A} \\pi(a|s) \\left( R_{s}^a + \\gamma \\sum_{s' \\in S} P_{ss'}^{a} v_k(s') \\right)\nAt each iteration \\(k + 1\\), update \\(v_{k + 1}(s)\\) from \\(v_{k}(s')\\), for all states \\(s \\in \\mathcal{S}\\). Here, \\(s'\\) is a successor state of \\(s\\).\nBackup diagram\n\n\nImage1\nThe gridworld consists of a 4x4 grid with 15 states, where each cell represents a state. There is one terminal state (shown as shaded squares), and the agent receives a reward of -1 until it reaches the terminal state. The agent follows a uniform random policy, where it has an equal probability of 0.25 to move in any of the four directions (up, down, left, or right) from each state.\n\\pi(n|.) = \\pi(e|.) = \\pi(s|.) = \\pi(w|.) = 0.25\nActions that would lead the agent out of the grid leave the state unchanged. The goal is to evaluate the performance of this random policy in terms of the undiscounted episodic MDP, where the discount factor \\(\\gamma\\) is set to 1, indicating that future rewards are not discounted.\nImage 2"
  },
  {
    "objectID": "blogs/Policy-Iteration-and-Value-Iteration.html#policy-iteration",
    "href": "blogs/Policy-Iteration-and-Value-Iteration.html#policy-iteration",
    "title": "Policy Iteration and Value Iteration",
    "section": "",
    "text": "Given an initial policy \\(\\pi\\), the process of policy improvement involves evaluating the policy and then improving it iteratively. The policy evaluation step estimates the value function \\(v_{\\pi}(s)\\) for each state s under the current policy \\(\\pi\\).\nv_{\\pi}(s) = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + ... | S_t = s]\nThe policy improvement step then updates the policy to a new policy \\(\\pi'\\) by acting greedily with respect to the value function \\(v_{\\pi}\\).\n\\pi' = greedy(v_{\\pi})\nDiagram\nIn the gridworld example, the policy was optimal, \\(\\pi' = \\pi_{\\ast}\\). The process of policy iteration always converges to \\(\\pi_{\\ast}\\).\n\n\nThe problem involves managing a car rental business with two locations, where each location has a maximum capacity of 20 cars. The states in this problem represent the number of cars available at each location. The actions involve moving up to 5 cars between the two locations overnight to rebalance the inventory based on demand. The reward is \\(\\$10\\) for each car rented (assuming a car is available at the requested location). The transitions, or the dynamics of the environment, are determined by the random returns and requests for cars at each location. These transitions follow a Poisson distribution, n returns/requests with prob \\(\\frac{{e^{-\\lambda } \\lambda^n }}{{n!}}\\), where: - At the first location, the average number of car returns and requests is 3. - At the second location, the average number of car requests is 4, and the average number of car returns is 2.\nThe goal is to find an optimal policy for moving cars between the two locations to maximize the overall expected reward (revenue from rented cars).\nDiagram\nThe plots show the progression of value functions \\((v_0, v_1, v_2)\\) and intermediate policies \\((\\pi_0, \\pi_1, \\pi_2, \\pi_3, \\pi_4)\\) during the iterations of policy evaluation and improvement.\nThe value functions are depicted as 3D surfaces, representing the expected future rewards for each combination of car counts at the two locations. The policies are visualized as 2D plots, indicating the optimal number of cars to move between locations for each state.\nStarting with an initial policy π0, the process alternates between evaluating the current policy to compute the value function and improving the policy greedily based on the value function.\nThe iterations continue until the policy converges to the optimal policy \\(\\pi^{\\ast}\\) that maximizes the expected rewards in the Car Rental problem.\n\n\n\nConsider a deterministic policy, $a = (s). We can improve the policy by acting greedily,\n\\pi'(s) = \\arg\\max_{a\\in\\mathcal{A}} q_\\pi(s, a)\nThis improves the value from any state \\(s\\) over one step,\nq_\\pi(s, \\pi'(s)) = \\max_{a\\in\\mathcal{A}} q_\\pi(s, a) \\geq q_\\pi(s, \\pi(s)) = v_\\pi(s)\nIt therefore improves the value function, \\(v_{\\pi'}(s) \\geq v_\\pi(s)\\)\n\\begin{align*}\nv_\\pi(s) &\\leq q_\\pi(s, \\pi'(s)) = \\mathbb{E}\\pi[R{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\\n&\\leq \\mathbb{E}\\pi[R{t+1} + \\gamma q_\\pi(S_{t+1},\\pi'(S_{t+1})) | S_t = s] \\\n&\\leq \\mathbb{E}\\pi[R{t+1} + \\gamma R_{t+2} + \\gamma^2 q_\\pi(S_{t+2},\\pi'(S_{t+2})) | S_t = s] \\\n&\\leq \\mathbb{E}\\pi[R{t+1} + \\gamma R_{t+2} + \\ldots | S_t = s] = v_{\\pi'}(s)\n\\end{align*}\nIf improvements stop,\nq_\\pi(s, \\pi'(s)) = \\max_{a\\in\\mathcal{A}} q_\\pi(s, a) = q_\\pi(s, \\pi(s)) = v_\\pi(s)\nThen, the Bellman optimality equation has been satisfied\nv_\\pi(s) = \\max_{a\\in\\mathcal{A}} q_\\pi(s, a)\nTherefore \\(v_\\pi(s) = v_*(s)\\) for all \\(s \\in \\mathcal{S}\\). So \\(\\pi\\) is an optimal policy."
  },
  {
    "objectID": "blogs/Policy-Iteration-and-Value-Iteration.html#value-iteration",
    "href": "blogs/Policy-Iteration-and-Value-Iteration.html#value-iteration",
    "title": "Policy Iteration and Value Iteration",
    "section": "",
    "text": "Any optimal policy can be subdivided into two components: - An optimal first action \\(A\\), - Followed by an optimal policy from successor state \\(S'\\)\n\nTheorem (Principle of Optimality)\nA policy \\(\\pi(a|s)\\) achieves the optimal value from state \\(s\\), \\(v_\\pi(s) = v_(s)\\), if and only if - For any state \\(s'\\) reachable from \\(s\\) - \\(\\pi\\) achieves the optimal value from state \\(s'\\), \\(v_\\pi(s') = v_(s')\\)\n\n\n\n\nValue iteration is a technique used to compute the optimal value function v*(s) for a given Markov Decision Process (MDP). The idea is to iteratively update the value function using the Bellman equation until convergence.\nIf we know the solution to the subproblems \\(v_{\\ast}(s')\\), i.e., the optimal values for the next states \\(s'\\), then the optimal value \\(v_{\\ast}(s)\\) for the current state s can be found by a one-step lookahead:\nv_{\\ast}(s) \\leftarrow \\max_{a \\in A} R^a_s + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_{\\ast}(s')\nThe value iteration algorithm applies these updates iteratively, starting with an initial approximation and updating the values until convergence.\nThe intuition behind value iteration is to start with the final rewards and work backwards, computing the optimal values by considering the immediate rewards and the discounted future rewards from the next states.\nAlthough derived for deterministic MDPs, value iteration can also be applied to stochastic (loopy) MDPs.\n\n\n\nProblem: Find the optimal policy \\(\\pi\\)\nWe apply the Bellman Optimality Equation with iterative backups.\nv_{k + 1}(s) = \\max_{a \\in A} \\left[R_{s}^a + \\gamma \\sum_{s' \\in S}  P_{ss'}^a v_{k}(s')\\right]\nAt each iteration \\(k + 1\\), update \\(v_{k + 1}(s)\\) from \\(v_{k}(s')\\), for all states \\(s \\in \\mathcal{S}\\), using synchronous backups.\nDiagram"
  },
  {
    "objectID": "blogs/Policy-Iteration-and-Value-Iteration.html#summary-synchronous-dynamic-programming-algorithms",
    "href": "blogs/Policy-Iteration-and-Value-Iteration.html#summary-synchronous-dynamic-programming-algorithms",
    "title": "Policy Iteration and Value Iteration",
    "section": "",
    "text": "Image"
  },
  {
    "objectID": "blogs/Policy-Iteration-and-Value-Iteration.html#references",
    "href": "blogs/Policy-Iteration-and-Value-Iteration.html#references",
    "title": "Policy Iteration and Value Iteration",
    "section": "",
    "text": "[1]: Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.\n[2]: Markov Decision Processes, Subir Varma.\n[3]: Markov Decision Processes, David Silver, UCL Course on RL."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html",
    "href": "blogs/Markov-Decision-Processes.html",
    "title": "Markov Decision Processes",
    "section": "",
    "text": "Reinforcement Learning is a sub-domain of machine learning where a learner called an agent interacts with its surroundings called environment. In return, the environment provides rewards and a new state determined by the actions of the agent."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#the-agentenvironment-interface",
    "href": "blogs/Markov-Decision-Processes.html#the-agentenvironment-interface",
    "title": "Markov Decision Processes",
    "section": "The Agent–Environment Interface",
    "text": "The Agent–Environment Interface\n\nThe learner and decision-maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment.\n\nPlaceholder for Image\nThe environment refers to the aspects of the problem that the agent cannot directly control or manipulate. The agent’s actions, on the other hand, are the decisions that the agent is tasked with learning to make in order to maximize some reward signal.\nThe state of the environment encompasses all the information that may be useful for the agent in choosing its actions. Notably, the agent is not assumed to be completely ignorant of the environment. For example, the agent may have some knowledge about how its actions and the resulting states lead to the calculation of rewards. However, even though the agent understands this reward function, it is still considered part of the environment because the agent cannot arbitrarily change it.\nThis distinction between what the agent knows and what it can control is a critical concept in reinforcement learning. An agent may have a thorough understanding of the problem domain, much like a human who knows the rules of solving a Rubik’s cube but still struggles actually to find the optimal solution. The agent-environment relationship, therefore, represents the fundamental limitations on the agent’s control rather than just the limitations on its knowledge."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#the-markov-property",
    "href": "blogs/Markov-Decision-Processes.html#the-markov-property",
    "title": "Markov Decision Processes",
    "section": "The Markov Property",
    "text": "The Markov Property\n\n“The future is independent of the past given the future.”\n\nThe agent makes its decisions as a function of a signal from the environment called the environment’s state. Ideally, a state signal should summarise past actions compactly yet in such a way that all relevant information is retained. This normally requires more than the immediate actions but never more than the complete history of all past sensations. A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property.\nMathematically, the Markov property can be expressed as,\n\\[\nP[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1}, ..., S_{t}]\n\\]\n\nMarkov Processes\nA Markov Process is a memoryless random process, i.e. a sequence of random states \\(S_{1}, S_{2}, ...\\) with the Markov property. A Markov Process can be represented as a tuple \\(\\langle S, \\textit{P} \\rangle\\), where \\(S\\) is a finite set of states and P is the transition state probability matrix, P\\(ss'\\) \\(= P[S_{t+1} = s' | S_{t} = s]\\).\n\n\nState Transition Probability\nFor Markov state \\(s\\) and successor state \\(s'\\), the state transition probability is defined by,\n\\[ {P}_{ss'} = P[S_{t+1} = s' | S_{t} = s] \\]\nThe state transition matrix P defines transition probabilities from all states to all successor states.\n\\[\n{P} = \\begin{bmatrix}\n    \\textit{P}_{11} & \\dots & \\textit{P}_{1n} \\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\textit{P}_{n1} & \\dots & \\textit{P}_{nn}\n    \\end{bmatrix}\n\\] ### Example of Markov chain\nImage Placeholder"
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#reward",
    "href": "blogs/Markov-Decision-Processes.html#reward",
    "title": "Markov Decision Processes",
    "section": "Reward",
    "text": "Reward\n\nAll of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n\nIn simpler terms, it is a numerical value given to the agent based on some action at some state in the environment."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#return",
    "href": "blogs/Markov-Decision-Processes.html#return",
    "title": "Markov Decision Processes",
    "section": "Return",
    "text": "Return\nIf the sequence of rewards received after time step \\(t\\) is denoted \\(R_{t+1}, R_{t+2}, R_{t+3}, . . .\\), we seek to maximize the expected return, where the return \\(G_t\\) is defined as some specific function of the reward sequence.\n\\[ G_t = R_{t+1} + R_{t+2} + R_{t+3} + · · · + R_T,\\text{ where T is a final time step} \\]"
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#episodic-and-continuing-tasks",
    "href": "blogs/Markov-Decision-Processes.html#episodic-and-continuing-tasks",
    "title": "Markov Decision Processes",
    "section": "Episodic and Continuing Tasks",
    "text": "Episodic and Continuing Tasks\n\nThe agent–environment interaction breaks naturally into subsequences, which we call episodes. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Tasks with episodes of this kind are called episodic tasks.\n\nHowever, this is not always the case.\n\nIn many cases the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. We call these continuing tasks."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#discount-rate",
    "href": "blogs/Markov-Decision-Processes.html#discount-rate",
    "title": "Markov Decision Processes",
    "section": "Discount Rate",
    "text": "Discount Rate\n\nThe discount rate determines the present value of future rewards.\n\nThe agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses \\(A_t\\) to maximize the expected discounted return:\n\\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\]\n\\(\\gamma\\) is a parameter, \\(0 \\leq \\gamma \\leq 1\\), called the discount rate. It determines the importance given to future rewards."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#markov-decision-processes",
    "href": "blogs/Markov-Decision-Processes.html#markov-decision-processes",
    "title": "Markov Decision Processes",
    "section": "Markov Decision Processes",
    "text": "Markov Decision Processes\n\nA reinforcement learning task that satisfies the Markov property is called a Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP).\n\nA Markov Decision Process can be represented as a tuple \\(\\langle S, A, \\textit{P}, R, \\gamma \\rangle\\) where, - \\(S\\) is a finite set of states * \\(A\\) is a finite set of actions * P is a state transition matrix, \\(P^a_{ss'} = \\mathbb{P}\\left[S_{t+1} = s' | S_t = s, A_t = a\\right]\\) * \\(R\\) is a reward function, \\(R^a_s = \\mathbb{E}\\left[R_{t+1} | S_t = s, A_t = a\\right]\\) * \\(\\gamma\\) is a discount rate, \\(\\gamma \\in [0,1]\\)"
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#policies",
    "href": "blogs/Markov-Decision-Processes.html#policies",
    "title": "Markov Decision Processes",
    "section": "Policies",
    "text": "Policies\n\nA policy \\(\\pi\\) is a probability distribution over actions given states.\n\n\\[ \\pi (a|s) = \\mathbb{P}\\left[A_t = a|S_t = s\\right] \\]\n\nA policy defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states."
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#value-function",
    "href": "blogs/Markov-Decision-Processes.html#value-function",
    "title": "Markov Decision Processes",
    "section": "Value Function",
    "text": "Value Function\n\nValue Functions estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of “how good” here is defined in terms of expected return.\n\n\nState-Value Function\nThe state-value function \\(V_{\\pi}(s)\\) of an MDP is the expected return starting from a state \\(s\\) under a policy \\(\\pi\\). \\[ V_{\\pi}(s) = \\mathbb{E}\\left[G_t|S_t = s\\right] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s \\right] \\]\n\n\nAction-Value Function\nThe action-value function \\(q_{\\pi}(s, a)\\) is the expected return starting from \\(s\\), taking the action \\(a\\), and thereafter following policy \\(\\pi\\).\n\\[ q_\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\, \\middle| \\, S_t = s, A_t = a \\right] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\middle| \\, S_t = s, A_t = a \\right] \\]"
  },
  {
    "objectID": "blogs/Markov-Decision-Processes.html#references",
    "href": "blogs/Markov-Decision-Processes.html#references",
    "title": "Markov Decision Processes",
    "section": "References",
    "text": "References\n\n\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.\nMarkov Decision Processes, Subir Varma\nJ. Norris, “Markov Chains”, Cambridge University Press, Cambridge, 1998."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RL-Team",
    "section": "",
    "text": "This is the homepage for the RL-Team. We are a group of enturepeneurs, students and faculty members who are interested in Reinforcement Learning.\nCome join us in our learning journey!"
  }
]