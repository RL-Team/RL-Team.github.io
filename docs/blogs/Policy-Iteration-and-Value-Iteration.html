<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>RL-Team – policy-iteration-and-value-iteration</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">RL-Team</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blogs.html" rel="" target="">
 <span class="menu-text">Blogs</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#policy-iteration-and-value-iteration" id="toc-policy-iteration-and-value-iteration" class="nav-link active" data-scroll-target="#policy-iteration-and-value-iteration">Policy Iteration and Value Iteration</a>
  <ul class="collapse">
  <li><a href="#dynamic-programming" id="toc-dynamic-programming" class="nav-link" data-scroll-target="#dynamic-programming">Dynamic Programming</a>
  <ul class="collapse">
  <li><a href="#planning-by-dynamic-programming" id="toc-planning-by-dynamic-programming" class="nav-link" data-scroll-target="#planning-by-dynamic-programming">Planning by Dynamic Programming</a></li>
  </ul></li>
  <li><a href="#iterative-policy-evaluation" id="toc-iterative-policy-evaluation" class="nav-link" data-scroll-target="#iterative-policy-evaluation">Iterative Policy Evaluation</a>
  <ul class="collapse">
  <li><a href="#example-gridworld" id="toc-example-gridworld" class="nav-link" data-scroll-target="#example-gridworld">Example (Gridworld):</a></li>
  </ul></li>
  <li><a href="#policy-iteration" id="toc-policy-iteration" class="nav-link" data-scroll-target="#policy-iteration">Policy Iteration</a>
  <ul class="collapse">
  <li><a href="#example-jacks-car-rental" id="toc-example-jacks-car-rental" class="nav-link" data-scroll-target="#example-jacks-car-rental">Example (Jack’s Car Rental):</a></li>
  <li><a href="#policy-improvement" id="toc-policy-improvement" class="nav-link" data-scroll-target="#policy-improvement">Policy Improvement</a></li>
  </ul></li>
  <li><a href="#value-iteration" id="toc-value-iteration" class="nav-link" data-scroll-target="#value-iteration">Value Iteration</a>
  <ul class="collapse">
  <li><a href="#principle-of-optimality" id="toc-principle-of-optimality" class="nav-link" data-scroll-target="#principle-of-optimality">Principle of Optimality</a></li>
  <li><a href="#deterministic-value-iteration" id="toc-deterministic-value-iteration" class="nav-link" data-scroll-target="#deterministic-value-iteration">Deterministic Value Iteration</a></li>
  <li><a href="#value-iteration-1" id="toc-value-iteration-1" class="nav-link" data-scroll-target="#value-iteration-1">Value Iteration</a></li>
  </ul></li>
  <li><a href="#summary-synchronous-dynamic-programming-algorithms" id="toc-summary-synchronous-dynamic-programming-algorithms" class="nav-link" data-scroll-target="#summary-synchronous-dynamic-programming-algorithms">Summary (Synchronous Dynamic Programming Algorithms)</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="policy-iteration-and-value-iteration" class="level1">
<h1>Policy Iteration and Value Iteration</h1>
<section id="dynamic-programming" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-programming">Dynamic Programming</h2>
<blockquote class="blockquote">
<p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).</p>
</blockquote>
<p>Dynamic programming involves solving complex problems by breaking them down into sub-problems. Dynamic programming is a very general solution method for problems that have two properties,</p>
<ul>
<li><strong>Optimal Substructure</strong>: The optimal solution to a dynamic optimisation problem can be found by combining the optimal solutions to its sub-problems. This is known as the <em>Principle of Optimality</em>. Optimal solutions can be decomposed into subproblems.</li>
<li><strong>Overlapping subproblems</strong>: Sub-problems can recur many times. The solution of one sub-problem is cached and reused to solve recursive sub-problems.</li>
</ul>
<p>Markov decision processes satisfy both properties,</p>
<ul>
<li><p>Bellman equation gives recursive decomposition. The equation breaks down finding the value function of a state by dividing it into sub-problems.</p>
<pre class="math"><code>v_\pi(s) = \mathbb{E}_\pi [R_{t+1} + \gamma V_\pi(S_{t+1}) | S_t = s]</code></pre></li>
<li><p>Value function stores and reuses solutions.</p></li>
</ul>
<section id="planning-by-dynamic-programming" class="level3">
<h3 class="anchored" data-anchor-id="planning-by-dynamic-programming">Planning by Dynamic Programming</h3>
<p>Dynamic programming assumes full knowledge of the MDP. It is used for <em>planning</em> in an MDP.</p>
<p>For Prediction, - Input: MDP <span class="math inline">\(\langle S, A, \textit{P}, R, \gamma \rangle\)</span> and policy <span class="math inline">\(\pi\)</span> <strong>OR</strong> MRP <span class="math inline">\(\langle S, \textit{P}^{\pi}, R^{\pi}, \gamma \rangle\)</span> - Output: value function <span class="math inline">\(v_\pi\)</span></p>
<p>For Control: - Input: MDP <span class="math inline">\(\langle S, A, \textit{P}, R, \gamma \rangle\)</span> - Output: optimal value function <span class="math inline">\(v_{\ast}\)</span> and optimal policy <span class="math inline">\(\pi_{\ast}\)</span>.</p>
</section>
</section>
<section id="iterative-policy-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="iterative-policy-evaluation">Iterative Policy Evaluation</h2>
<p>How do we evaluate an arbitrary policy <span class="math inline">\(\pi\)</span>?</p>
<p>We apply the Bellman Expectation Equation with iterative backups. We calculate the value of the next state by backing up the value of the current state from the previous iteration.</p>
<pre class="math"><code>v_{k+1}(s) = \sum_{a \in A} \pi(a|s) \left( R_{s}^a + \gamma \sum_{s' \in S} P_{ss'}^{a} v_k(s') \right)</code></pre>
<p>At each iteration <span class="math inline">\(k + 1\)</span>, update <span class="math inline">\(v_{k + 1}(s)\)</span> from <span class="math inline">\(v_{k}(s')\)</span>, for all states <span class="math inline">\(s \in \mathcal{S}\)</span>. Here, <span class="math inline">\(s'\)</span> is a successor state of <span class="math inline">\(s\)</span>.</p>
<p><em>Backup diagram</em></p>
<section id="example-gridworld" class="level3">
<h3 class="anchored" data-anchor-id="example-gridworld">Example (Gridworld):</h3>
<p><em>Image1</em></p>
<p>The gridworld consists of a 4x4 grid with 15 states, where each cell represents a state. There is one terminal state (shown as shaded squares), and the agent receives a reward of -1 until it reaches the terminal state. The agent follows a uniform random policy, where it has an equal probability of 0.25 to move in any of the four directions (up, down, left, or right) from each state.</p>
<pre class="math"><code>\pi(n|.) = \pi(e|.) = \pi(s|.) = \pi(w|.) = 0.25</code></pre>
<p>Actions that would lead the agent out of the grid leave the state unchanged. The goal is to evaluate the performance of this random policy in terms of the undiscounted episodic MDP, where the discount factor <span class="math inline">\(\gamma\)</span> is set to 1, indicating that future rewards are not discounted.</p>
<p><em>Image 2</em></p>
</section>
</section>
<section id="policy-iteration" class="level2">
<h2 class="anchored" data-anchor-id="policy-iteration">Policy Iteration</h2>
<p>Given an initial policy <span class="math inline">\(\pi\)</span>, the process of policy improvement involves evaluating the policy and then improving it iteratively. The policy evaluation step estimates the value function <span class="math inline">\(v_{\pi}(s)\)</span> for each state s under the current policy <span class="math inline">\(\pi\)</span>.</p>
<pre class="math"><code>v_{\pi}(s) = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + ... | S_t = s]</code></pre>
<p>The policy improvement step then updates the policy to a new policy <span class="math inline">\(\pi'\)</span> by acting greedily with respect to the value function <span class="math inline">\(v_{\pi}\)</span>.</p>
<pre class="math"><code>\pi' = greedy(v_{\pi})</code></pre>
<p><em>Diagram</em></p>
<p>In the gridworld example, the policy was optimal, <span class="math inline">\(\pi' = \pi_{\ast}\)</span>. The process of policy iteration always converges to <span class="math inline">\(\pi_{\ast}\)</span>.</p>
<section id="example-jacks-car-rental" class="level3">
<h3 class="anchored" data-anchor-id="example-jacks-car-rental">Example (Jack’s Car Rental):</h3>
<p>The problem involves managing a car rental business with two locations, where each location has a maximum capacity of 20 cars. The states in this problem represent the number of cars available at each location. The actions involve moving up to 5 cars between the two locations overnight to rebalance the inventory based on demand. The reward is <span class="math inline">\(\$10\)</span> for each car rented (assuming a car is available at the requested location). The transitions, or the dynamics of the environment, are determined by the random returns and requests for cars at each location. These transitions follow a Poisson distribution, n returns/requests with prob <span class="math inline">\(\frac{{e^{-\lambda } \lambda^n }}{{n!}}\)</span>, where: - At the first location, the average number of car returns and requests is 3. - At the second location, the average number of car requests is 4, and the average number of car returns is 2.</p>
<p>The goal is to find an optimal policy for moving cars between the two locations to maximize the overall expected reward (revenue from rented cars).</p>
<p><em>Diagram</em></p>
<p>The plots show the progression of value functions <span class="math inline">\((v_0, v_1, v_2)\)</span> and intermediate policies <span class="math inline">\((\pi_0, \pi_1, \pi_2, \pi_3, \pi_4)\)</span> during the iterations of policy evaluation and improvement.</p>
<p>The value functions are depicted as 3D surfaces, representing the expected future rewards for each combination of car counts at the two locations. The policies are visualized as 2D plots, indicating the optimal number of cars to move between locations for each state.</p>
<p>Starting with an initial policy π0, the process alternates between evaluating the current policy to compute the value function and improving the policy greedily based on the value function.</p>
<p>The iterations continue until the policy converges to the optimal policy <span class="math inline">\(\pi^{\ast}\)</span> that maximizes the expected rewards in the Car Rental problem.</p>
</section>
<section id="policy-improvement" class="level3">
<h3 class="anchored" data-anchor-id="policy-improvement">Policy Improvement</h3>
<p>Consider a deterministic policy, $a = (s). We can improve the policy by acting greedily,</p>
<pre class="math"><code>\pi'(s) = \arg\max_{a\in\mathcal{A}} q_\pi(s, a)</code></pre>
<p>This improves the value from any state <span class="math inline">\(s\)</span> over one step,</p>
<pre class="math"><code>q_\pi(s, \pi'(s)) = \max_{a\in\mathcal{A}} q_\pi(s, a) \geq q_\pi(s, \pi(s)) = v_\pi(s)</code></pre>
<p>It therefore improves the value function, <span class="math inline">\(v_{\pi'}(s) \geq v_\pi(s)\)</span></p>
<pre class="math"><code>\begin{align*}
v_\pi(s) &amp;\leq q_\pi(s, \pi'(s)) = \mathbb{E}\pi[R{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \
&amp;\leq \mathbb{E}\pi[R{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1})) | S_t = s] \
&amp;\leq \mathbb{E}\pi[R{t+1} + \gamma R_{t+2} + \gamma^2 q_\pi(S_{t+2},\pi'(S_{t+2})) | S_t = s] \
&amp;\leq \mathbb{E}\pi[R{t+1} + \gamma R_{t+2} + \ldots | S_t = s] = v_{\pi'}(s)
\end{align*}</code></pre>
<p>If improvements stop,</p>
<pre class="math"><code>q_\pi(s, \pi'(s)) = \max_{a\in\mathcal{A}} q_\pi(s, a) = q_\pi(s, \pi(s)) = v_\pi(s)</code></pre>
<p>Then, the Bellman optimality equation has been satisfied</p>
<pre class="math"><code>v_\pi(s) = \max_{a\in\mathcal{A}} q_\pi(s, a)</code></pre>
<p>Therefore <span class="math inline">\(v_\pi(s) = v_*(s)\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span>. So <span class="math inline">\(\pi\)</span> is an optimal policy.</p>
</section>
</section>
<section id="value-iteration" class="level2">
<h2 class="anchored" data-anchor-id="value-iteration">Value Iteration</h2>
<section id="principle-of-optimality" class="level3">
<h3 class="anchored" data-anchor-id="principle-of-optimality">Principle of Optimality</h3>
<p>Any optimal policy can be subdivided into two components: - An optimal first action <span class="math inline">\(A\)</span>, - Followed by an optimal policy from successor state <span class="math inline">\(S'\)</span></p>
<blockquote class="blockquote">
<p>Theorem (Principle of Optimality)</p>
<p>A policy <span class="math inline">\(\pi(a|s)\)</span> achieves the optimal value from state <span class="math inline">\(s\)</span>, <span class="math inline">\(v_\pi(s) = v_(s)\)</span>, if and only if - For any state <span class="math inline">\(s'\)</span> reachable from <span class="math inline">\(s\)</span> - <span class="math inline">\(\pi\)</span> achieves the optimal value from state <span class="math inline">\(s'\)</span>, <span class="math inline">\(v_\pi(s') = v_(s')\)</span></p>
</blockquote>
</section>
<section id="deterministic-value-iteration" class="level3">
<h3 class="anchored" data-anchor-id="deterministic-value-iteration">Deterministic Value Iteration</h3>
<p>Value iteration is a technique used to compute the optimal value function v*(s) for a given Markov Decision Process (MDP). The idea is to iteratively update the value function using the Bellman equation until convergence.</p>
<p>If we know the solution to the subproblems <span class="math inline">\(v_{\ast}(s')\)</span>, i.e., the optimal values for the next states <span class="math inline">\(s'\)</span>, then the optimal value <span class="math inline">\(v_{\ast}(s)\)</span> for the current state s can be found by a one-step lookahead:</p>
<pre class="math"><code>v_{\ast}(s) \leftarrow \max_{a \in A} R^a_s + \gamma \sum_{s' \in S} P_{ss'}^a v_{\ast}(s')</code></pre>
<p>The value iteration algorithm applies these updates iteratively, starting with an initial approximation and updating the values until convergence.</p>
<p>The intuition behind value iteration is to start with the final rewards and work backwards, computing the optimal values by considering the immediate rewards and the discounted future rewards from the next states.</p>
<p>Although derived for deterministic MDPs, value iteration can also be applied to stochastic (loopy) MDPs.</p>
</section>
<section id="value-iteration-1" class="level3">
<h3 class="anchored" data-anchor-id="value-iteration-1">Value Iteration</h3>
<p>Problem: Find the optimal policy <span class="math inline">\(\pi\)</span></p>
<p>We apply the Bellman Optimality Equation with iterative backups.</p>
<pre class="math"><code>v_{k + 1}(s) = \max_{a \in A} \left[R_{s}^a + \gamma \sum_{s' \in S}  P_{ss'}^a v_{k}(s')\right]</code></pre>
<p>At each iteration <span class="math inline">\(k + 1\)</span>, update <span class="math inline">\(v_{k + 1}(s)\)</span> from <span class="math inline">\(v_{k}(s')\)</span>, for all states <span class="math inline">\(s \in \mathcal{S}\)</span>, using synchronous backups.</p>
<p><em>Diagram</em></p>
</section>
</section>
<section id="summary-synchronous-dynamic-programming-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="summary-synchronous-dynamic-programming-algorithms">Summary (Synchronous Dynamic Programming Algorithms)</h2>
<p><em>Image</em></p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>[1]: Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.</p>
<p>[2]: <a href="https://subirvarma.github.io/GeneralCognitics/Course2/Lecture2_MDPs.pdf">Markov Decision Processes, Subir Varma</a>.</p>
<p>[3]: <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf">Markov Decision Processes, David Silver, UCL Course on RL</a>.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>