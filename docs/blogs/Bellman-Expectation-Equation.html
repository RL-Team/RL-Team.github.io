<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-04-05">

<title>RL-Team - Bellman Expectation Equation and Optimality</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">RL-Team</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blogs.html" rel="" target="">
 <span class="menu-text">Blogs</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#bellman-expectation-equation" id="toc-bellman-expectation-equation" class="nav-link active" data-scroll-target="#bellman-expectation-equation">Bellman Expectation Equation</a></li>
  <li><a href="#optimal-value-function" id="toc-optimal-value-function" class="nav-link" data-scroll-target="#optimal-value-function">Optimal Value Function</a>
  <ul class="collapse">
  <li><a href="#optimal-state-value-function" id="toc-optimal-state-value-function" class="nav-link" data-scroll-target="#optimal-state-value-function">Optimal State Value Function</a></li>
  <li><a href="#optimal-action-value-function" id="toc-optimal-action-value-function" class="nav-link" data-scroll-target="#optimal-action-value-function">Optimal Action Value Function</a></li>
  </ul></li>
  <li><a href="#optimal-policy" id="toc-optimal-policy" class="nav-link" data-scroll-target="#optimal-policy">Optimal Policy</a>
  <ul class="collapse">
  <li><a href="#finding-an-optimal-policy" id="toc-finding-an-optimal-policy" class="nav-link" data-scroll-target="#finding-an-optimal-policy">Finding an Optimal Policy:</a></li>
  </ul></li>
  <li><a href="#bellman-optimality-equation" id="toc-bellman-optimality-equation" class="nav-link" data-scroll-target="#bellman-optimality-equation">Bellman Optimality Equation</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Bellman Expectation Equation and Optimality</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Fundamentals</div>
    <div class="quarto-category">Reinforcement Learning</div>
    <div class="quarto-category">Bellman Expectation Equation</div>
    <div class="quarto-category">Optimality</div>
    <div class="quarto-category">Mathematics</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 5, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>How do we calculate the value functions <span class="math inline">\(v_\pi (s)\)</span> and <span class="math inline">\(q_\pi (s,a)\)</span>, given a policy <span class="math inline">\(\pi\)</span>?</p>
<section id="bellman-expectation-equation" class="level2">
<h2 class="anchored" data-anchor-id="bellman-expectation-equation">Bellman Expectation Equation</h2>
<p>The state-value function can be decomposed into immediate reward plus the discounted value of the successor state,</p>
<p><span class="math display">\[ v_\pi(s) = \mathbb{E}_\pi [R_{t+1} + \gamma V_\pi(S_{t+1}) | S_t = s] \]</span></p>
<p>The above equation is the Bellman expectation equation for the state-value function.</p>
<p>The action-value function similarly decomposed,</p>
<p><span class="math display">\[ q_\pi(s, a) = \mathbb{E}\pi [R{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \]</span></p>
<p>The above equation is the Bellman expectation equation for the action-value function.</p>
<p><em>Back-up diagram state value</em></p>
<p>This backup diagram describes the value of being in a particular state.</p>
<p>Mathematically, this can be represented as,</p>
<p><span class="math display">\[ v(s) = R_s + \gamma \sum_{s' \in S} P_{ss'} v(s') \]</span></p>
<p>Similarly,</p>
<p><em>backup diagram for action value</em></p>
<p>This backup diagram shows us “how good” it is to take an action in a given state under a given policy.</p>
<p>Mathematically, this can be represented as,</p>
<p><span class="math display">\[ q_\pi(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s')$ \]</span></p>
<p>If we connect and extend both the back-up diagrams to further define <span class="math inline">\(v_\pi (s)\)</span>,</p>
<p><em>extended state value backup diagram</em></p>
<p>Mathematically, this can be represented as,</p>
<p><span class="math display">\[ v_\pi(s) = \sum_{a \in A} \pi(a|s) \left(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s')\right) \]</span></p>
<p>Example:</p>
<p><em>Example</em></p>
<p>Similarly, if we connect and extend both the back-up diagrams to further define <span class="math inline">\(q_\pi (s, a)\)</span>,</p>
<p><em>extended action value backup diagram</em></p>
<p>Mathematically, this can be represented as,</p>
<p><span class="math display">\[ q_{\pi}(s, a) = R_s^a + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a'|s') q_{\pi}(s', a') \]</span></p>
<p>Example:</p>
<p><em>Example</em></p>
</section>
<section id="optimal-value-function" class="level2">
<h2 class="anchored" data-anchor-id="optimal-value-function">Optimal Value Function</h2>
<p>The Bellman Expectation Equations evaluate how good a state is for a particular policy. However, they do not tell us the optimal policy.</p>
<section id="optimal-state-value-function" class="level3">
<h3 class="anchored" data-anchor-id="optimal-state-value-function">Optimal State Value Function</h3>
<p>The <strong>optimal state-value function</strong> <span class="math inline">\(v_{*} (s)\)</span> is the maximum state-value function for all policies</p>
<p><span class="math display">\[ v_{*}(s) = \mathop{\max}_\pi v_\pi(s) \]</span></p>
<p><span class="math inline">\(v_{*} (s)\)</span> tells us the maximum reward that can be extracted from the system when starting in state s. However, it doesn’t tell us what policy to follow.</p>
</section>
<section id="optimal-action-value-function" class="level3">
<h3 class="anchored" data-anchor-id="optimal-action-value-function">Optimal Action Value Function</h3>
<p>The <strong>optimal action-value function</strong> <span class="math inline">\(q_{*} (s, a)\)</span> is the maximum action-value function for all policies</p>
<p><span class="math display">\[ q_{*}(s, a) = \mathop{\max}_\pi q_\pi(s, a) \]</span></p>
<p><span class="math inline">\(q_{*} (s, a)\)</span> tells us the maximum reward that can be extracted from the system if action a is taken while in state s.</p>
</section>
</section>
<section id="optimal-policy" class="level2">
<h2 class="anchored" data-anchor-id="optimal-policy">Optimal Policy</h2>
<p>In order to find an optimal policy, we have to define the notion of optimality. What does it mean for one policy to be better than another?</p>
<p>A partial ordering over policies can be defined using the concept of value functions. The partial ordering states that one policy is better than another if the value function of the first policy is greater than or equal to the value function of the second policy for all states. This partial ordering relationship is formally expressed in the equation,</p>
<p><span class="math display">\[ \pi \geq \pi' \text{ if } v_\pi(s) \geq v_{\pi'}(s), \forall s \]</span></p>
<blockquote class="blockquote">
<p>Theorem:</p>
<p>For any Markov Decision Process, 1. There exists an optimal policy <span class="math inline">\(\pi_{\ast}\)</span> that is better than or equal to all other policies, <span class="math inline">\(\pi_{\ast} \geq \pi, \forall \pi\)</span> 2. All optimal policies achieve the optimal value function, <span class="math inline">\(v_{\pi_{\ast}}(s) = v_{\ast}(s)\)</span> 3. All optimal policies achieve the optimal action-value function, <span class="math inline">\(q_{\pi_{\ast}}(s, a) = q_{\ast}(s, a)\)</span></p>
</blockquote>
<section id="finding-an-optimal-policy" class="level3">
<h3 class="anchored" data-anchor-id="finding-an-optimal-policy">Finding an Optimal Policy:</h3>
<p>An optimal policy can be found by maximising over <span class="math inline">\(q_{\ast}(s, a)\)</span>,</p>
<p><span class="math display">\[ \pi_{\ast}(a|s) = \begin{cases}
1 &amp; \text{if } a = \arg\underset{a \in \mathcal{A}}\max q_*(s, a) \\
0 &amp; \text{otherwise}
\end{cases} \]</span></p>
<ul>
<li>There is always a deterministic optimal policy for any MDP.</li>
<li>If we know <span class="math inline">\(q_{\ast}(s,a )\)</span>, we immediately have the optimal policy.</li>
</ul>
<p>Example: <em>EXAMPLE IMAGE</em></p>
</section>
</section>
<section id="bellman-optimality-equation" class="level2">
<h2 class="anchored" data-anchor-id="bellman-optimality-equation">Bellman Optimality Equation</h2>
<p><em>Back Diagram state-value</em></p>
<p>Suppose the agent is in state S, and from that state, it can take two actions (a). Instead of using the Bellman Expectation Equation to calculate the value of being in state S by taking the average of the action-values, the agent chooses the action with the greater <span class="math inline">\(q_{\ast}\)</span> value. This gives the agent the value of being in state s.</p>
<p><span class="math display">\[ v_{\ast}(s) = \mathop{\max}_a q_{\ast}(s, a) \]</span></p>
<p>Similarly,</p>
<p><em>Backup diagram action-value</em></p>
<p>Suppose the agent has taken an action a in some state s. The environment then transitions the agent to a new state s’, which could be any of the possible next states. In this case, rather than using the Bellman Expectation Equation, which would involve taking the average of the values of the possible next states, the agent uses the Bellman Optimality Equation.</p>
<p><span class="math display">\[ q_{\ast}(s, a) = R_s^a + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a v_{\ast}(s') \]</span></p>
<p>If we connect and extend both the back-up diagrams,</p>
<p><em>extended state value backup diagram</em></p>
<p>An agent in a particular state ‘s’ will take an action a based on a policy that assigns weighted probabilities to the available actions. This action then leads the agent to end up in one of several possible next states s’, with the probabilities of ending up in each state s determined by the weighted environment. To find the value of being in the original state s, we simply average the optimal values of all the possible next states s’. This average gives us the overall value of being in state s - it tells us how good it is for the agent to be in that state, taking into account the weighted probabilities of the available actions and the resulting next states.</p>
<p><span class="math display">\[ v_{\ast}(s) = \mathop{\max}_a (R_s^a + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a v_{\ast}(s')) \]</span></p>
<p>There is no dependency on the policy anymore; it is solely a function of the environment’s randomness.</p>
<p><em>extended action value backup diagram</em></p>
<p>Suppose our agent is in a particular state s. The agent takes an action a in that state, which may result in the agent ending up in any of several possible next states s’. From each of these possible next states, the agent wants to identify the action with the highest <span class="math inline">\(q_{\ast}\)</span> value, i.e.&nbsp;the action that will maximize the expected future reward. The agent then backs this information up to the current state s, which allows the agent to determine the overall value of taking the original action a from the current state. In this way, the agent is able to choose the action that will lead to the highest expected future reward by considering the possible future states and the best actions to take in each of them.</p>
<p><span class="math display">\[ q_{\ast}(s, a) = R_s^a + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \mathop{\max}_{a'}q_{\ast}(s', a') \]</span></p>
<p><em>Example</em></p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><p>Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). The MIT Press.</p></li>
<li><p><a href="https://subirvarma.github.io/GeneralCognitics/Course2/Lecture2_MDPs.pdf">Markov Decision Processes, Subir Varma</a>.</p></li>
<li><p><a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf">Markov Decision Processes, David Silver, UCL Course on RL</a>.</p></li>
<li><p>J. Norris, “Markov Chains,” Cambridge University Press, Cambridge, 1998.</p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'alternate';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>